{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Graphs\n",
    "\n",
    "In this notebook, some simple computational graph (CG) nodes are implemented. Each with a forward pass method (prop_forward) and a backward pass method (prop_backward). No assumption of being in a neural network is present, so the gradient is calculated considering all values to be instances of a variable of interest, this is, there is no concept or weights or data points. There are just inputs and outputs. The reason for this approach is that I consider it important to learn about computational graphs in order to understand neural networks better. After all, neural networks are not anything but that: ~~snowflaked~~ computational graphs\n",
    "\n",
    "The assumption above does not impede nor complicate, however, the construction of a neural network, because a wrapper class (*e.g.* a Perceptron class) can use these CG components and be aware of weights and data. Then, such wrapper classes can be grouped in different ways to create more complex neural structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGNode(ABC):\n",
    "    \n",
    "    '''\n",
    "    Computational graph node template. In all of the methods,\n",
    "    the input vectors must have a shape of the form (n,)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.last_input = None\n",
    "        self.last_output = None\n",
    "    \n",
    "    @abstractmethod\n",
    "    def prop_forward(self):\n",
    "        \n",
    "        '''\n",
    "        Take inputs and evaluate the\n",
    "        node's corresponding operation\n",
    "        '''\n",
    "\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def prop_backward(self, grad):\n",
    "        \n",
    "        '''\n",
    "        Evaluate the gradient of the node's\n",
    "        corresponding operation using an \n",
    "        incoming gradient 'grad' in backprop\n",
    "        '''\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGSum(CGNode):\n",
    "    \n",
    "    '''\n",
    "    Computational graph sum node\n",
    "    '''\n",
    "    \n",
    "    def prop_forward(self, x):\n",
    "        self.last_input = np.array(x)\n",
    "        self.last_output = np.sum(x)\n",
    "        return self.last_output\n",
    "    \n",
    "    def prop_backward(self, grad):\n",
    "        return grad * np.ones(self.last_input.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.2, 1.2, 1.2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = CGSum()\n",
    "s.prop_forward([1,7,9])\n",
    "s.prop_backward(1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGMul(CGNode):\n",
    "    \n",
    "    '''\n",
    "    Computational graph multiplication node\n",
    "    '''\n",
    "    \n",
    "    def prop_forward(self, x):\n",
    "        self.last_input = np.array(x)\n",
    "        self.last_output = np.prod(self.last_input)\n",
    "        return self.last_output\n",
    "    \n",
    "    def prop_backward(self, grad):        \n",
    "        # repeat x size times and arrange as rows\n",
    "        \n",
    "        size = self.last_input.size\n",
    "        \n",
    "        repeated = np.ones((size, size))*self.last_input\n",
    "        \n",
    "        # start differentiation\n",
    "        np.fill_diagonal(repeated, 1)\n",
    "        \n",
    "        # multiply constants \n",
    "        # (variables not being differentiated)\n",
    "        return grad * np.prod(repeated, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = CGMul()\n",
    "a.prop_forward([1,2,3,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([48., 24., 16.,  6.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.prop_backward(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGExp(CGNode):\n",
    "    \n",
    "    def prop_forward(self, x):\n",
    "        self.last_input = np.array(x)\n",
    "        self.last_output = np.exp(self.last_input)\n",
    "        return self.last_output\n",
    "    \n",
    "    def prop_backward(self, grad):\n",
    "        return grad * self.last_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGDot:\n",
    "    \n",
    "    '''\n",
    "    Dot product node composed of summation and multiplication nodes for \n",
    "    transparency and accordance with the concept of a computational graph\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.last_x = None\n",
    "        self.last_y = None\n",
    "        self.last_output = None\n",
    "        \n",
    "        # one summation\n",
    "        self.cg_sum = None\n",
    "        \n",
    "        # many multiplications\n",
    "        self.cg_muls = None\n",
    "    \n",
    "    def prop_forward(self, x, y):\n",
    "        \n",
    "        '''Compute the dot product of vectors x and y'''\n",
    "        \n",
    "        self.last_x = np.array(x)\n",
    "        self.last_y = np.array(y)\n",
    "        \n",
    "        # if no data has been propagated yet,\n",
    "        # instantiate as many mul nodes\n",
    "        # as the dimension of the inputs\n",
    "        if not self.cg_sum:\n",
    "            self.cg_muls = [CGMul() for _ in self.last_x]\n",
    "            self.cg_sum = CGSum()\n",
    "        \n",
    "        # align corresponding values to multiply\n",
    "        xy = CGDot.pre_dot(x, y)\n",
    "        \n",
    "        # multiplication phase\n",
    "        summands = [cg_mul.prop_forward(factors) for cg_mul, factors in zip(self.cg_muls, xy)]\n",
    "        \n",
    "        # summation\n",
    "        self.last_output = self.cg_sum.prop_forward(summands)\n",
    "        \n",
    "        return self.last_output\n",
    "    \n",
    "    def prop_backward(self, grad, flat=False):\n",
    "        \n",
    "        '''\n",
    "        Propagate the gradients backward by calling prop_backward\n",
    "        of the sum and multiplications of which the dot product is \n",
    "        composed of. Actual computation is delegated to smaller \n",
    "        nodes\n",
    "        \n",
    "        :returns: nx2 array (n being the size of the input). Each\n",
    "        row is the gradient of a sum of two input vector components\n",
    "        unless flat is True, in which case the return value is just\n",
    "        the flattened version of the nx2 array\n",
    "        '''\n",
    "        \n",
    "        sum_grad = self.cg_sum.prop_backward(grad)\n",
    "        mul_grad = np.array([\n",
    "            cg_mul.prop_backward(sum_grad_i) for cg_mul, sum_grad_i in zip(self.cg_muls, sum_grad)\n",
    "        ])\n",
    "        \n",
    "        return mul_grad.flatten() if flat else mul_grad\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def pre_dot(x, y):\n",
    "        \n",
    "        '''\n",
    "        Arrange the inputs to facilitate \n",
    "        dot product computation\n",
    "        '''\n",
    "        \n",
    "        x = np.reshape(x, (1, len(x)))\n",
    "        y = np.reshape(y, (1, len(y)))\n",
    "        xy = np.concatenate((x,y))\n",
    "        return xy.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CGDot in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot = CGDot()\n",
    "dot.prop_forward([1,2,3], [4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 1.],\n",
       "       [5., 2.],\n",
       "       [6., 3.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot.prop_backward(grad=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
